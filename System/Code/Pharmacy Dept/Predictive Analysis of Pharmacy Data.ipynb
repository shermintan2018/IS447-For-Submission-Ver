{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pharmacy Department - Predictive Analysis\n",
    "\n",
    "Analyzing through various concepts of forecasting for No. of TTOs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../../Data/Pharmacy Dept/Data Cleaning/Merged_Pharmacy_Dept_CLEANED.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change datetime data format \n",
    "df[['Date']] = df[['Date']].apply(pd.to_datetime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create df with daily number of TTO "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_no_tto_df = df.groupby(['Date']).size().reset_index(name='TTO Count')\n",
    "#daily_no_tto_df = df.groupby(['Date', 'Hospital']).size().reset_index(name='TTO Count')\n",
    "daily_no_tto_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_no_tto_hosp_df = df.groupby(['Date', 'Hospital']).size().reset_index(name='TTO Count')\n",
    "daily_no_tto_hosp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check info\n",
    "daily_no_tto_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test to fix SARIMA\n",
    "daily_no_tto_df = daily_no_tto_df.replace(np.inf, np.nan).replace(-np.inf, np.nan).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_no_tto_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change 'Date' column to datetime\n",
    "daily_no_tto_df['Date'] = pd.to_datetime(daily_no_tto_df['Date'], infer_datetime_format=True)\n",
    "#Set as index\n",
    "daily_no_tto_df = daily_no_tto_df.set_index('Date')\n",
    "\n",
    "daily_no_tto_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exclude jan 2022 data since it is from MEH only \n",
    "daily_no_tto_df = daily_no_tto_df['2021-01-01':'2021-12-31'].resample('W').sum()\n",
    "daily_no_tto_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check for Common Time Series Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualize the Data\n",
    "y = daily_no_tto_df['TTO Count']\n",
    "fig, ax = plt.subplots(figsize=(20,6))\n",
    "\n",
    "ax.plot(y, marker='.', linestyle='-', linewidth=0.5, label='Weekly')\n",
    "ax.plot(y.resample('M').mean(), marker='o', markersize=8, linestyle='-', label='Monthly Mean Resample')\n",
    "ax.set_ylabel('TTO Count')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decompose Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#graphs to show seasonal_decompose \n",
    "#max period is round down(53/2) \n",
    "def seasonal_decompose(y):\n",
    "    decomposition = sm.tsa.seasonal_decompose(y, model='additive', extrapolate_trend='freq', period=7)\n",
    "    fig = decomposition.plot()\n",
    "    fig.set_size_inches(14,7)\n",
    "    plt.show()\n",
    "\n",
    "seasonal_decompose(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot for Rolling Statistic to test Stationarity\n",
    "def test_stationarity(timeseries, title):\n",
    "\n",
    "    #Determine rolling statistics\n",
    "    rolmean = pd.Series(timeseries).rolling(window=12).mean()\n",
    "    rolstd = pd.Series(timeseries).rolling(window=12).std()\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(16, 4))\n",
    "    ax.plot(timeseries, label= title)\n",
    "    ax.plot(rolmean, label='rolling mean')\n",
    "    ax.plot(rolstd, label='rolling std (x10)')\n",
    "    ax.legend()\n",
    "    \n",
    "pd.options.display.float_format = '{:.8f}'.format\n",
    "test_stationarity(y, 'raw data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augmented Dickey-Fuller Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "statistical test used to test whether a given Time series is stationary or not\n",
    "\n",
    "Null Hypothesis (H0): If failed to be rejected, it suggests the time series has a unit root, meaning it is non-stationary. It has some time dependent structure.\n",
    "Alternate Hypothesis (H1): The null hypothesis is rejected; it suggests the time series does not have a unit root, meaning it is stationary. It does not have time-dependent structure.\n",
    "\n",
    "p-value > 0.05: Fail to reject the null hypothesis (H0), the data has a unit root and is non-stationary.\n",
    "\n",
    "p-value <= 0.05: Reject the null hypothesis (H0), the data does not have a unit root and is stationary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import adfuller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Augmented Dickey-Fuller Test\n",
    "def ADF_test(timeseries, dataDesc):\n",
    "    print(' > Is the {} stationary ?'.format(dataDesc))\n",
    "    dftest = adfuller(timeseries.dropna(), autolag='AIC')\n",
    "    print('Test statistic = {:.3f}'.format(dftest[0]))\n",
    "    print('P-value = {:.3f}'.format(dftest[1]))\n",
    "    print('Critical values :')\n",
    "    for k, v in dftest[4].items():\n",
    "        print('\\t{}: {} - The data is {} stationary with {}% confidence'.format(k, v, 'not' if v<dftest[0] else '', 100-int(k[:-1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ADF_test(y, 'raw data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make the data stationary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#detrend\n",
    "y_detrend = (y - y.rolling(window=12).mean())/y.rolling(window=12).std()\n",
    "\n",
    "test_stationarity(y_detrend,'de-trended data')\n",
    "ADF_test(y_detrend,'de-trended data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#differencing\n",
    "y_12lag = y - y.shift(12)\n",
    "\n",
    "test_stationarity(y_12lag,'12 lag differenced data')\n",
    "ADF_test(y_12lag,'12 lag differenced data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set y_to_train, y_to_test, and the length of predict units\n",
    "y_to_train = y[:'2021-06-30'] #dataset to train\n",
    "y_to_test = y['2021-07-01':] #last X months for test\n",
    "\n",
    "predict_date = len(y) - len(y[:'2021-06-30']) # the number of data points for the test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Exponential Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from statsmodels.tsa.api import SimpleExpSmoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ses(y, y_to_train,y_to_test,smoothing_level,predict_date):\n",
    "    y.plot(marker='o', color='black', legend=True, figsize=(14, 7))\n",
    "    \n",
    "    fit1 = SimpleExpSmoothing(y_to_train).fit(smoothing_level=smoothing_level,optimized=False)\n",
    "    fcast1 = fit1.forecast(predict_date).rename(r'$\\alpha={}$'.format(smoothing_level))\n",
    "    # specific smoothing level\n",
    "    fcast1.plot(marker='o', color='blue', legend=True)\n",
    "    fit1.fittedvalues.plot(marker='o',  color='blue')\n",
    "    mse1 = ((fcast1 - y_to_test) ** 2).mean()\n",
    "    print('The Root Mean Squared Error of our forecasts with smoothing level of {} is {}'.format(smoothing_level,round(np.sqrt(mse1), 2)))\n",
    "    \n",
    "    ## auto optimization\n",
    "    fit2 = SimpleExpSmoothing(y_to_train).fit()\n",
    "    fcast2 = fit2.forecast(predict_date).rename(r'$\\alpha=%s$'%fit2.model.params['smoothing_level'])\n",
    "    # plot\n",
    "    fcast2.plot(marker='o', color='green', legend=True)\n",
    "    fit2.fittedvalues.plot(marker='o', color='green')\n",
    "    \n",
    "    mse2 = ((fcast2 - y_to_test) ** 2).mean()\n",
    "    print('The Root Mean Squared Error of our forecasts with auto optimization is {}'.format(round(np.sqrt(mse2), 2)))\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "ses(y, y_to_train,y_to_test,0.8,predict_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpretation:\n",
    " shows the difference between the specified α (blue line) and the auto-optimized α (green line). As you can see from the graph, SES will predict a flat, forecasted line since the logic behind it uses weighted averages. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Holt's Linear Trend Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.bounteous.com/insights/2020/09/15/forecasting-time-series-model-using-python-part-two/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.api import Holt\n",
    "\n",
    "def holt(y,y_to_train,y_to_test,smoothing_level,smoothing_slope, predict_date):\n",
    "    y.plot(marker='o', color='black', legend=True, figsize=(14, 7))\n",
    "    \n",
    "    fit1 = Holt(y_to_train).fit(smoothing_level, smoothing_slope, optimized=False)\n",
    "    fcast1 = fit1.forecast(predict_date).rename(\"Holt's linear trend\")\n",
    "    mse1 = ((fcast1 - y_to_test) ** 2).mean()\n",
    "    print('The Root Mean Squared Error of Holt''s Linear trend {}'.format(round(np.sqrt(mse1), 2)))\n",
    "\n",
    "    fit2 = Holt(y_to_train, exponential=True).fit(smoothing_level, smoothing_slope, optimized=False)\n",
    "    fcast2 = fit2.forecast(predict_date).rename(\"Exponential trend\")\n",
    "    mse2 = ((fcast2 - y_to_test) ** 2).mean()\n",
    "    print('The Root Mean Squared Error of Holt''s Exponential trend {}'.format(round(np.sqrt(mse2), 2)))\n",
    "    \n",
    "    fit1.fittedvalues.plot(marker=\"o\", color='blue')\n",
    "    fcast1.plot(color='blue', marker=\"o\", legend=True)\n",
    "    fit2.fittedvalues.plot(marker=\"o\", color='red')\n",
    "    fcast2.plot(color='red', marker=\"o\", legend=True)\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "holt(y, y_to_train,y_to_test,0.6,0.2,predict_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SARIMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sarima \n",
    "def sarima_grid_search(y,seasonal_period):\n",
    "    p = d = q = range(0, 2)\n",
    "    pdq = list(itertools.product(p, d, q))\n",
    "    seasonal_pdq = [(x[0], x[1], x[2],seasonal_period) for x in list(itertools.product(p, d, q))]\n",
    "    \n",
    "    mini = float('+inf')\n",
    "    \n",
    "    \n",
    "    for param in pdq:\n",
    "        for param_seasonal in seasonal_pdq:\n",
    "            try:\n",
    "                mod = sm.tsa.statespace.SARIMAX(y,\n",
    "                                                order=param,\n",
    "                                                seasonal_order=param_seasonal,\n",
    "                                                enforce_stationarity=False,\n",
    "                                                enforce_invertibility=False)\n",
    "\n",
    "                results = mod.fit()\n",
    "                \n",
    "                if results.aic < mini:\n",
    "                    mini = results.aic\n",
    "                    param_mini = param\n",
    "                    param_seasonal_mini = param_seasonal\n",
    "\n",
    "#                 print('SARIMA{}x{} - AIC:{}'.format(param, param_seasonal, results.aic))\n",
    "            except:\n",
    "                continue\n",
    "    print('The set of parameters with the minimum AIC is: SARIMA{}x{} - AIC:{}'.format(param_mini, param_seasonal_mini, mini))\n",
    "\n",
    "sarima_grid_search(y,12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The grid search tested all possible combinations of variables, and printed out the set that resulted in the lowest AIC, and we can see that SARIMA(1, 1, 0)x(1, 1, 0, 12) has the lowest AIC value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call this function after pick the right(p,d,q) for SARIMA based on AIC               \n",
    "def sarima_eva(y,order,seasonal_order,seasonal_period,pred_date,y_to_test):\n",
    "    # fit the model \n",
    "    mod = sm.tsa.statespace.SARIMAX(y,\n",
    "                                order=order,\n",
    "                                seasonal_order=seasonal_order,\n",
    "                                enforce_stationarity=False,\n",
    "                                enforce_invertibility=False)\n",
    "\n",
    "    results = mod.fit()\n",
    "    print(results.summary().tables[1])\n",
    "    \n",
    "    results.plot_diagnostics(figsize=(16, 8))\n",
    "    plt.show()\n",
    "    \n",
    "    # The dynamic=False argument ensures that we produce one-step ahead forecasts, \n",
    "    # meaning that forecasts at each point are generated using the full history up to that point.\n",
    "    pred = results.get_prediction(start=pd.to_datetime(pred_date), dynamic=False)\n",
    "    pred_ci = pred.conf_int()\n",
    "    y_forecasted = pred.predicted_mean\n",
    "    mse = ((y_forecasted - y_to_test) ** 2).mean()\n",
    "    print('The Root Mean Squared Error of SARIMA with season_length={} and dynamic = False {}'.format(seasonal_period,round(np.sqrt(mse), 2)))\n",
    "\n",
    "    ax = y.plot(label='observed')\n",
    "    y_forecasted.plot(ax=ax, label='One-step ahead Forecast', alpha=.7, figsize=(14, 7))\n",
    "    ax.fill_between(pred_ci.index,\n",
    "                    pred_ci.iloc[:, 0],\n",
    "                    pred_ci.iloc[:, 1], color='k', alpha=.2)\n",
    "\n",
    "    ax.set_xlabel('Date')\n",
    "    ax.set_ylabel('Calls Offered')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # A better representation of our true predictive power can be obtained using dynamic forecasts. \n",
    "    # In this case, we only use information from the time series up to a certain point, \n",
    "    # and after that, forecasts are generated using values from previous forecasted time points.\n",
    "    pred_dynamic = results.get_prediction(start=pd.to_datetime(pred_date), dynamic=True, full_results=True)\n",
    "    pred_dynamic_ci = pred_dynamic.conf_int()\n",
    "    y_forecasted_dynamic = pred_dynamic.predicted_mean\n",
    "    mse_dynamic = ((y_forecasted_dynamic - y_to_test) ** 2).mean()\n",
    "    print('The Root Mean Squared Error of SARIMA with season_length={} and dynamic = True {}'.format(seasonal_period,round(np.sqrt(mse_dynamic), 2)))\n",
    "\n",
    "    ax = y.plot(label='observed')\n",
    "    y_forecasted_dynamic.plot(label='Dynamic Forecast', ax=ax,figsize=(14, 7))\n",
    "    ax.fill_between(pred_dynamic_ci.index,\n",
    "                    pred_dynamic_ci.iloc[:, 0],\n",
    "                    pred_dynamic_ci.iloc[:, 1], color='k', alpha=.2)\n",
    "\n",
    "    ax.set_xlabel('Date')\n",
    "    ax.set_ylabel('Calls Offered')\n",
    "\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    return (results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sarima_eva(y,(1,1,0),(1,1,0,12),12,'2021-07-04',y_to_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forecast(model,predict_steps,y):\n",
    "    \n",
    "    pred_uc = model.get_forecast(steps=predict_steps)\n",
    "\n",
    "    #SARIMAXResults.conf_int, can change alpha,the default alpha = .05 returns a 95% confidence interval.\n",
    "    pred_ci = pred_uc.conf_int()\n",
    "\n",
    "    ax = y.plot(label='observed', figsize=(14, 7))\n",
    "#     print(pred_uc.predicted_mean)\n",
    "    pred_uc.predicted_mean.plot(ax=ax, label='Forecast')\n",
    "    ax.fill_between(pred_ci.index,\n",
    "                    pred_ci.iloc[:, 0],\n",
    "                    pred_ci.iloc[:, 1], color='k', alpha=.25)\n",
    "    ax.set_xlabel('Date')\n",
    "    ax.set_ylabel(y.name)\n",
    "\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    # Produce the forcasted tables \n",
    "    pm = pred_uc.predicted_mean.reset_index()\n",
    "    pm.columns = ['Date','Predicted_Mean']\n",
    "    pci = pred_ci.reset_index()\n",
    "    pci.columns = ['Date','Lower Bound','Upper Bound']\n",
    "    final_table = pm.join(pci.set_index('Date'), on='Date')\n",
    "    \n",
    "    return (final_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_table = forecast(model,52,y)\n",
    "final_table.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating SARIMA with MAPE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each predicted data point, the absolute difference from the corresponding test point was calculated, and then divided by the test point. The average percentage gives the MAPE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = sm.tsa.statespace.SARIMAX(y,order = (1,1,0), \n",
    "                                seasonal_order= (1,1,0,12),\n",
    "                                seasonal_period= 12)\n",
    "results = mod.fit()\n",
    "print(results.summary().tables[1])\n",
    "    \n",
    "results.plot_diagnostics(figsize=(16, 8))\n",
    "#plt.show()\n",
    "\n",
    "    # The dynamic=False argument ensures that we produce one-step ahead forecasts, \n",
    "    # meaning that forecasts at each point are generated using the full history up to that point.\n",
    "pred = results.get_prediction(start=pd.to_datetime('2021-07-04'), dynamic=False)\n",
    "pred_ci = pred.conf_int()\n",
    "y_forecasted = pred.predicted_mean\n",
    "y_forecasted\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import acf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forecast_accuracy(forecast, actual):\n",
    "    mape = np.mean(np.abs(forecast - actual)/np.abs(actual))  # mean absolute percentage error\n",
    "    me = np.mean(forecast - actual)             # ME\n",
    "    mae = np.mean(np.abs(forecast - actual))    # mean absolute error\n",
    "    mpe = np.mean((forecast - actual)/actual)   # MPE\n",
    "    rmse = np.mean((forecast - actual)**2)**.5  # root mean square\n",
    "    corr = np.corrcoef(forecast, actual)[0,1]   # corr\n",
    "    mins = np.amin(np.hstack([forecast[:,None], actual[:,None]]), axis=1)\n",
    "    maxs = np.amax(np.hstack([forecast[:,None], actual[:,None]]), axis=1)\n",
    "    minmax = 1 - np.mean(mins/maxs)             # minmax\n",
    "    acf1 = acf(forecast-y_to_test)[1]                      # ACF1\n",
    "    return({'mape':mape, 'me':me, 'mae': mae, \n",
    "            'mpe': mpe, 'rmse':rmse, 'acf1':acf1,\n",
    "            'corr':corr, 'minmax':minmax})\n",
    "\n",
    "forecast_accuracy(y_forecasted, y_to_test.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Around 2.246% MAPE implies the model is about 97.754% accurate in predicting the next 28 observations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ARIMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "register_matplotlib_converters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_no_tto_df.head()\n",
    "\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('TTO Count')\n",
    "plt.plot(daily_no_tto_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import adfuller\n",
    "def test_stationarity(timeseries, window = 12, cutoff = 0.01):\n",
    "\n",
    "    #Determing rolling statistics\n",
    "    rolmean = timeseries.rolling(window).mean()\n",
    "    rolstd = timeseries.rolling(window).std()\n",
    "\n",
    "    #Plot rolling statistics:\n",
    "    fig = plt.figure(figsize=(12, 8))\n",
    "    orig = plt.plot(timeseries, color='blue',label='Original')\n",
    "    mean = plt.plot(rolmean, color='red', label='Rolling Mean')\n",
    "    std = plt.plot(rolstd, color='black', label = 'Rolling Std')\n",
    "    plt.legend(loc='best')\n",
    "    plt.title('Rolling Mean & Standard Deviation')\n",
    "    plt.show()\n",
    "    \n",
    "    #Perform Dickey-Fuller test:\n",
    "    print('Results of Dickey-Fuller Test:')\n",
    "    dftest = adfuller(timeseries, autolag='AIC', maxlag = 20 )\n",
    "    dfoutput = pd.Series(dftest[0:4], index=['Test Statistic','p-value','#Lags Used','Number of Observations Used'])\n",
    "    for key,value in dftest[4].items():\n",
    "        dfoutput['Critical Value (%s)'%key] = value\n",
    "    pvalue = dftest[1]\n",
    "    if pvalue < cutoff:\n",
    "        print('p-value = %.10f. The series is likely stationary.' % pvalue)\n",
    "    else:\n",
    "        print('p-value = %.10f. The series is likely non-stationary.' % pvalue)\n",
    "    \n",
    "    print(dfoutput)\n",
    "    \n",
    "test_stationarity(daily_no_tto_df['TTO Count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_diff = daily_no_tto_df['TTO Count'] - daily_no_tto_df['TTO Count'].shift(1)\n",
    "first_diff = first_diff.dropna(inplace = False)\n",
    "test_stationarity(first_diff, window = 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "fig = plt.figure(figsize=(12,8))\n",
    "ax1 = fig.add_subplot(211)\n",
    "fig = sm.graphics.tsa.plot_acf(daily_no_tto_df['TTO Count'], lags=40, ax=ax1) # \n",
    "ax2 = fig.add_subplot(212)\n",
    "fig = sm.graphics.tsa.plot_pacf(daily_no_tto_df['TTO Count'], lags=40, ax=ax2)# , lags=40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12,8))\n",
    "ax1 = fig.add_subplot(211)\n",
    "fig = sm.graphics.tsa.plot_acf(first_diff, lags=40, ax=ax1)\n",
    "ax2 = fig.add_subplot(212)\n",
    "fig = sm.graphics.tsa.plot_pacf(first_diff, lags=40, ax=ax2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
